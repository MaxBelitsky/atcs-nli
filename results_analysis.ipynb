{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxbelitsky/miniconda3/envs/acts-nli/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[2024-04-19 17:08:17,952] [INFO] [datasets:config.py:58] PyTorch version 2.2.2 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils import (\n",
    "    read_glove_embeddings,\n",
    "    build_tokenizer,\n",
    "    compute_metrics,\n",
    "    evaluate_model,\n",
    "    load_checkpoint_weights,\n",
    ")\n",
    "from src.data import get_dataset, CustomCollator\n",
    "from src.models import LSTMEmbedder, BiLSTMEmbedder, BiLSTMPooledEmbedder, SentenceClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the models on NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-19 17:08:26,995] [INFO] [torchtext.vocab.vectors:vectors.py:172] Loading vectors from .vector_cache/glove.840B.300d.txt.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OrderedVocab you are attempting to save contains holes for indices [138702], your vocabulary could be corrupted !\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "batch_size = 64\n",
    "mlp_hidden_dim = 512\n",
    "\n",
    "EMBEDDERS = {\n",
    "    \"lstm\": LSTMEmbedder,\n",
    "    \"bi-lstm\": BiLSTMEmbedder,\n",
    "    \"bi-lstm-pool\": BiLSTMPooledEmbedder\n",
    "    }\n",
    "CHECKPOINTS = {\n",
    "    \"lstm\": \"lstm_2024_04_17_13_47.pt\",\n",
    "    \"bi-lstm\": \"bi-lstm_2024_04_17_13_53.pt\",\n",
    "    \"bi-lstm-pool\": \"bi-lstm-pool_2024_04_17_14_52.pt\"\n",
    "}\n",
    "\n",
    "dataset = get_dataset()\n",
    "words, vectors = read_glove_embeddings()\n",
    "\n",
    "tokenizer = build_tokenizer(words)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset['validation'],\n",
    "    collate_fn=CustomCollator(tokenizer, device),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset['test'],\n",
    "    collate_fn=CustomCollator(tokenizer, device),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstm</td>\n",
       "      <td>0.810678</td>\n",
       "      <td>0.805296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bi-lstm</td>\n",
       "      <td>0.805459</td>\n",
       "      <td>0.803166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bi-lstm-pool</td>\n",
       "      <td>0.848669</td>\n",
       "      <td>0.844663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  val_accuracy  test_accuracy\n",
       "0          lstm      0.810678       0.805296\n",
       "1       bi-lstm      0.805459       0.803166\n",
       "2  bi-lstm-pool      0.848669       0.844663"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "for embedder_name, embedder_class in EMBEDDERS.items():\n",
    "\n",
    "    # Initialize the models\n",
    "    embedder = embedder_class(vectors)\n",
    "    model = SentenceClassificationModel(embedder, mlp_hidden_dim, 3).to(device)\n",
    "\n",
    "    # Load weights\n",
    "    load_checkpoint_weights(model, f\"models/{CHECKPOINTS[embedder_name]}\", device, skip_glove=True)\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_metrics = evaluate_model(model, val_dataloader)\n",
    "    test_metrics = evaluate_model(model, test_dataloader)\n",
    "\n",
    "    # Collect the metrics\n",
    "    results = {\n",
    "        \"model\": embedder_name,\n",
    "        \"val_accuracy\": val_metrics['accuracy'].item(),\n",
    "        \"test_accuracy\": test_metrics['accuracy'].item()\n",
    "    }\n",
    "    all_results.append(results)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models with a custom hypothesis and premisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(model, premise, hypothesis):\n",
    "    tokenized_premise = tokenizer(premise, return_tensors=\"pt\").to(device)\n",
    "    tokenized_premise['length'] = tokenized_premise['attention_mask'].sum(dim=1).cpu()\n",
    "\n",
    "    tokenized_hypothesis = tokenizer(hypothesis, return_tensors=\"pt\").to(device)\n",
    "    tokenized_hypothesis['length'] = tokenized_hypothesis['attention_mask'].sum(dim=1).cpu()\n",
    "\n",
    "    model_output = model(tokenized_premise, tokenized_hypothesis)\n",
    "    prediction = model_output.argmax(dim=1)\n",
    "    return prediction.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"Two men sitting in the sun\"\n",
    "hypothesis = \"Nobody is sitting in the shade\"\n",
    "\n",
    "predict_label(model, premise, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"A man is walking a dog\"\n",
    "hypothesis = \"No cat is outside\"\n",
    "\n",
    "predict_label(model, premise, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./models/lstm_2024_04_17_13_47.pt\"\n",
    "\n",
    "state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "model.embedder.lstm.state_dict()\n",
    "\n",
    "embedding_model_file = \"models/lstm_embedder.pt\"\n",
    "\n",
    "torch.save(model.embedder.lstm.state_dict(), embedding_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder.lstm.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('mps')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acts-nli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
